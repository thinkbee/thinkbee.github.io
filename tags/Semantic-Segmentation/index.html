<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>태그: Semantic Segmentation - IT Tech blogging</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="thinkbee blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="thinkbee blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Machine learning, Deep learning, Python, Data science and Mobile Programming like Android"><meta property="og:type" content="website"><meta property="og:title" content="IT Tech blogging"><meta property="og:url" content="https://thinkbee.github.io/"><meta property="og:site_name" content="IT Tech blogging"><meta property="og:description" content="Machine learning, Deep learning, Python, Data science and Mobile Programming like Android"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://thinkbee.github.io/img/og_image.png"><meta property="article:author" content="Gangtai Goh"><meta property="article:tag" content="Machine learning, Deep learning, Python, Data science, Android, Ubuntu, Linux, macOS"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://thinkbee.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thinkbee.github.io"},"headline":"IT Tech blogging","image":["https://thinkbee.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Gangtai Goh"},"publisher":{"@type":"Organization","name":"IT Tech blogging","logo":{"@type":"ImageObject","url":"https://thinkbee.github.io/images/thinkbee_logo.png"}},"description":"Machine learning, Deep learning, Python, Data science and Mobile Programming like Android"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/vs2015.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-5ZH90CZ414" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-5ZH90CZ414');</script><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-1465716454138955" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/thinkbee_logo.png" alt="IT Tech blogging" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Blog</a><a class="navbar-item" href="/documents">Doc</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/qkboo"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-four-fifths-tablet is-four-fifths-desktop is-four-fifths-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">태그</a></li><li class="is-active"><a href="#" aria-current="page">Semantic Segmentation</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-12-23T01:00:00.000Z" title="12/23/2024, 10:00:00 AM">2024-12-23</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2024-12-23T14:36:49.756Z" title="12/23/2024, 11:36:49 PM">2024-12-23</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a></span><span class="level-item">35분안에 읽기 (약 5197 단어)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Image_Segmentation-4f9d5782ee2d/">[요약] Image Segmentation</a></p><div class="content"><p>Image Segmentation:</p>
<p><a target="_blank" rel="noopener" href="https://storm.genie.stanford.edu/article/image-segmentation%0A-410216">https://storm.genie.stanford.edu/article/image-segmentation%0A-410216</a></p>
<p>이미지 분할(Image Sementation)의 주요 목적은 특정 영역 분리와 식별이다. 이미지 안의 여러 사물에서 관심 영역을 분리함으로써 사용자는 관련 정보를 추출하고 해당 영역을 정량적으로 분석할 수 있게 된다. </p>
<h1 id="이미지-분할-Image-Sementation-이란"><a href="#이미지-분할-Image-Sementation-이란" class="headerlink" title="이미지 분할 (Image Sementation) 이란"></a>이미지 분할 (Image Sementation) 이란</h1><p>이미지 분할 (Segmentation) 은 이미지 안의 개체를 분석에 필요한 대상을 다중 분할 또는 영역으로 나누는데 관련한 컴퓨터 비전과 인공지능의 중대한 기술이다. </p>
<p>이미지에 있는 뚜렷한 객체 또는 특성을 식별하고 분리하기 위해서 세그멘테이션은 의료 이미지에서 자동차까지 다양한 응용분야에서 성장했다.</p>
<p>이미지의 공간적 구조와 구성에 대한 자세한 정보를 제공하는 기능은 컴퓨터 비전에서 주목할 만한 분야로, 의료, 감시 및 엔터테인먼트 등의 산업 전반에 사용되고 있다.</p>
<p>이미지 분할의 진화는 1950년대와 1960년대의 초기 방법에서 시작하여 현대 기술의 토대를 마련한 몇 가지 중요한 단계를 거쳐 추적할 수 있다. [1][2]</p>
<p>이 분야는 1990년대 디지털 혁명 동안 디지털 이미지와 계산 능력의 증가로 인해 상당한 발전을 이루었다. 이는 2000년대와 2010년대에 정점을 이루었고, 특히 합성곱 신경망(CNN)과 같은 딥 러닝의 출현으로 이미지 분할 기능이 변형되어 이미지 내의 복잡한 패턴과 객체를 인식하는 데 있어 전례 없는 정확성과 효율성을 이루었다. [2] [3]</p>
<p>기술의 발전에 따라 이미지 분할은 감시와 얼굴 인식에서 윤리적 우려, 프라이버시 등의 의문이 제기되고 또한 데이터 가변성, 폐색 및 고급 알고리즘의 높은 계산 요구 사항과 같은 문제는 효과적인 구현에 대한 상당한 장애물로 계속 남아 있다.[1][3]</p>
<h1 id="헬름홀츠의-무의식적-추론-이론"><a href="#헬름홀츠의-무의식적-추론-이론" class="headerlink" title="헬름홀츠의 무의식적 추론 이론"></a>헬름홀츠의 무의식적 추론 이론</h1><p>19세기의 헤르만 폰 헬름홀츠는 우리가 보는 것은 단순히 망막에 맺힌 이미지가 아니라 뇌가 경험과 지식을 바탕으로 재구성한 결과라는 것이이다. 시각이론은 컴퓨터 비전 시스템이 상황 정보, 사전 지식, 추론 능력 등을 활용해야 함을 시사했습니다. [A4]</p>
<h1 id="초기-1950년대-1960년대"><a href="#초기-1950년대-1960년대" class="headerlink" title="초기 1950년대~1960년대"></a>초기 1950년대~1960년대</h1><p>1950년대에서 1960년대는 컴퓨터 비전과 인공 지능(AI)의 시작 단계였다. AI가 학문 분야로 공식적으로 확립된 시기였으며, 특히 존 매카시와 마빈 민스키와 같은 선구자들이 조직한 1956년 다트머스 컨퍼런스가 주요 성과였다. [1]</p>
<p>AI 초기 연구는 기계에 시각 데이터를 “보고” 해석하는 능력을 부여하는 데 중점을 두었으며, 이는 이미지 처리 및 세분화 기술의 미래 발전을 위한 토대를 마련했습니다.</p>
<p>디지털 이미지 처리는 1960년대에 이미지를 디지털 형식으로 변환하여 컴퓨터 처리가 가능해졌다. 1963년 MIT의 Larry Roberts는 2차원 이미지에서 3차원 장면을 재구성하는 알고리즘을 도입하여 이 분야를 발전시켰습니다. 이러한 발전으로 컴퓨터는 이미지 데이터를 분석하고 해석할 수 있게 되었습니다. [A3]</p>
<p>James Cooley와 John Tukey가 1965년에 개발한 고속 퓨리에 변환(FFT) 알고리즘 개발하였고, FFT는 이미지와 같은 신호를 공간 영역에서 주파수 영역으로 변환하는 이산 퓨리에 변환(DFT)을 효율적으로 계산한다. 컴퓨터 비전에서 FFT를 활용하여 이미지 필터링, 노이즈 감소, 특징 추출과 같은 필수 작업을 수행할 수 있다. </p>
<h1 id="성장과-도전-1980년대"><a href="#성장과-도전-1980년대" class="headerlink" title="성장과 도전 1980년대"></a>성장과 도전 1980년대</h1><p>DARPA 등 지원으로 연구자들은 이미지 분할의 기본 구성 요소인 객체 인식 및 장면 이해와 관련된 복잡한 과제를 해결하기 시작다. 그러나 당시 기술은 제한적이었으며 많은 시스템이 조명, 크기 및 관점의 변화로 인해 객체를 정확하게 식별하는 데 어려움을 겪었습니다.[1]</p>
<h1 id="디지털-혁명-1990년대"><a href="#디지털-혁명-1990년대" class="headerlink" title="디지털 혁명 1990년대"></a>디지털 혁명 1990년대</h1><p>1990년대는 디지털 카메라의 등장과 인터넷의 급속한 확장을 특징으로 하는 디지털 혁명으로 시각 데이터의 가용성이 기하급수적으로 증가하면서 컴퓨터 비전에서 디지털 이미지의 알고리즘을 훈련하기 위한 풍부한 데이터 세트를 제공되며 이미지 분할 방법론의 역량을 향상시켰다. 특히 객체 인식 및 특징 추출 기술에 대한 추가 연구 개발을 촉진했습니다. [2]</p>
<h1 id="빅데이터와-딥러닝의-부상-2000년대-2010년대"><a href="#빅데이터와-딥러닝의-부상-2000년대-2010년대" class="headerlink" title="빅데이터와 딥러닝의 부상(2000년대-2010년대)"></a>빅데이터와 딥러닝의 부상(2000년대-2010년대)</h1><p>2000년대는 빅데이터와 강력한 컴퓨팅 리소스에 의해 합성곱 신경망(CNN)의 도입으로 이미지 처리의 풍경을 변화시켜 시각적 인식 작업의 속도와 정확도를 크게 향상시켰습니다.[ 2 ]</p>
<p>2010년대에는 딥 러닝 혁명이 이미지 분할 기술을 더욱 발전시키며 얼굴 인식 시스템 및 자율 주행차를 포함한 다양한 분야에서 이미지 분할이 실질적으로 적용되었다.  </p>
<p>2012년에 CNN 기반 접근 방식이 ImageNet Large Scale Visual Recognition Challenge(ILSVRC)에서 놀라운 성공을 거두면서 이미지 인식 작업에서 딥 러닝의 혁신적 힘을 강조하면서 분수령이 발생했습니다. [1]</p>
<h1 id="현대적-이슈와-미래-방향"><a href="#현대적-이슈와-미래-방향" class="headerlink" title="현대적 이슈와 미래 방향"></a>현대적 이슈와 미래 방향</h1><p>이미지 분할 기술이 발전함에 따라 특히 감시 및 얼굴 인식 응용 분야의 맥락에서 윤리 및 개인 정보 보호 문제도 제기되었습니다.[ 1 ]</p>
<p>이러한 기술의 의미를 둘러싼 지속적인 논쟁은 기술 발전과 개인의 권리를 균형 있게 유지하는 책임 있는 혁신의 필요성을 강조합니다. 앞으로 적대적 훈련 및 전이 학습을 포함한 새로운 방법의 지속적인 개발은 이미지 분할 연구와 실제 세계 응용 분야에 대한 흥미로운 미래를 예고합니다.[ 1 ] [ 3 ]</p>
<h1 id="방법"><a href="#방법" class="headerlink" title="방법"></a>방법</h1><p>방법은 전통적인 기술과 딥 러닝 프레임워크를 사용하는 기술로 크게 분류할 수 있습니다.</p>
<h2 id="영역-확장-Region-growing-method"><a href="#영역-확장-Region-growing-method" class="headerlink" title="영역 확장 Region-growing method"></a>영역 확장 Region-growing method</h2><p>영역 확장 방법은 영역 내의 이웃 픽셀이 유사한 강도 값을 공유해서 시드 픽셀을 선택하고 유사성 기준을 충족하는 이웃 픽셀을 추가하여 영역을 확장하는 방법이다.</p>
<p><img src='https://users.cs.cf.ac.uk/dave/Vision_lecture/region_growing.gif'
    style='background-color:white'></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://users.cs.cf.ac.uk/dave/Vision_lecture/node35.html">https://users.cs.cf.ac.uk/dave/Vision_lecture/node35.html</a></li>
</ul>
<p>시드 영역 확장(SRG) 기술은 미리 정해진 시드 포인트를 사용하여 강도 유사성에 따라 영역을 반복적으로 확장하는 반면, 시드되지 않은 영역 확장 방법은 단일 픽셀로 시작하여 임계값 비교에 따라 새로운 영역을 만듭니다. [4][5]</p>
<h2 id="임계값-설정-Thresholding"><a href="#임계값-설정-Thresholding" class="headerlink" title="임계값 설정 Thresholding"></a>임계값 설정 Thresholding</h2><p>임계값 설정은 임계값을 사용하여 픽셀을 여러 클래스로 구분하는 기본 기술로, 객체 감지 및 특징 추출과 같은 작업을 가능하게 합니다. </p>
<p><img src='https://images.prismic.io/encord/48d82379-658d-4262-9190-34eae93d00b4_image3.png?auto=compress,format' 
 style='background-color:white'></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://encord.com/blog/image-thresholding-image-processing/">https://encord.com/blog/image-thresholding-image-processing/</a></li>
</ul>
<p>간단한 임계값 설정은 전역 임계값을 기반으로 각 픽셀에 이진 값을 할당하지만 다양한 조명 조건에서 어려움을 겪을 수 있습니다. 적응형 임계값 설정은 로컬 특성에 따라 각 픽셀의 임계값을 조정하여 복잡한 이미지의 분할을 개선합니다. [6]</p>
<h2 id="Motion-based-Segmentation"><a href="#Motion-based-Segmentation" class="headerlink" title="Motion-based Segmentation"></a>Motion-based Segmentation</h2><p>동작 기반 분할 기술은 일련의 이미지에서 동작 정보를 얻기 위해서 프레임 간의 차이점을 분석해서 움직이는 객체를 정적 배경에서 분리할 수 있습니다. </p>
<p><img src='https://ars.els-cdn.com/content/image/3-s2.0-B9780123744562000062-u05-09-9780123744562.jpg' 
 style='background-color:white'></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/motion-segmentation">https://www.sciencedirect.com/topics/engineering/motion-segmentation</a></li>
</ul>
<p>대화형 분할은 로봇 시스템을 활용하여 효과적인 분할에 필요한 동작 신호를 생성함으로써 이를 더욱 향상시킵니다.[7]</p>
<h2 id="딥러닝-U-Net-및-변형"><a href="#딥러닝-U-Net-및-변형" class="headerlink" title="딥러닝 U-Net 및 변형"></a>딥러닝 U-Net 및 변형</h2><p>u-net은 이미지를 빠르고 정확하게 분할하기 위한 합성곱 네트워크 아키텍처입니다. 원래 생물의학 이미지 분할을 위해 설계되어 이 분야의 기초 아키텍처가 되었다.  지금까지 전자 현미경 스택의 신경 구조 분할을 위한 ISBI 챌린지 에서 이전의 가장 좋은 방법(슬라이딩 윈도우 합성곱 네트워크)보다 더 나은 성과를 보였습니다. [A] </p>
<p>ISBI 2015에서 바이트윙 방사선 촬영에서 우식의 컴퓨터 자동 감지를 위한 그랜드 챌린지에서 우승했으며 , ISBI 2015에서 두 가지 가장 어려운 투과광 현미경 범주(위상차 및 DIC 현미경)에서 세포 추적 챌린지 에서 큰 차이로 우승했습니다 (또한 당사의 발표 참조 ).[A2]</p>
<p>U-Net 는 스킵 연결을 통해 공간 정보를 보존하면서 컨텍스트를 캡처하는 인코더-디코더 구조로 구성됩니다. </p>
<img src='https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png' style='background-color:white'>
 - https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/



<p>U-Net 모델의 변형 및 향상은 의료 이미지 분할 연구에서 널리 퍼져 있으며, 종종 다양한 애플리케이션에서 성능이 향상됩니다.[5][6]</p>
<h2 id="딥러닝-3D-CNN-및-Transformer-아키텍처"><a href="#딥러닝-3D-CNN-및-Transformer-아키텍처" class="headerlink" title="딥러닝 3D-CNN 및 Transformer 아키텍처"></a>딥러닝 3D-CNN 및 Transformer 아키텍처</h2><p>최근 이미지 분할의 발전으로 3D-CNN 및 Transformer 아키텍처를 기반으로 하는 모델은 체적 데이터 처리에서 강력한 성능을 보여주지만 일반적으로 상당한 계산 리소스가 필요합니다. 반면 Transformer 모델은 분할 도메인 내에서 혁신할 수 있는 잠재력으로 주목을 받고 있으며 복잡한 데이터 세트를 처리하는 새로운 접근 방식을 제공합니다. [8]</p>
<h2 id="결합된-분류기-Combind-Classifier-접근-방식"><a href="#결합된-분류기-Combind-Classifier-접근-방식" class="headerlink" title="결합된 분류기 (Combind Classifier) 접근 방식"></a>결합된 분류기 (Combind Classifier) 접근 방식</h2><p>여러 분류기의 통합은 세분화 정확도를 향상시키기 위해 제안되었습니다. 이 방법은 다양한 분류 알고리즘의 강점을 활용하여 인식 및 세분화 품질을 개선합니다. </p>
<p>의사결정 융합 전략의 계층 구조</p>
<img src='https://www.researchgate.net/profile/Sukhendu-Das/publication/45260893/figure/fig5/AS:670519506522116@1536875716261/A-hierarchy-of-methods-used-in-combining-classifiers-proposed-for-discussion-in-this.png' style='background-color:white'>
 - A Survey of Decision Fusion and Feature Fusion Strategies for Pattern Classification


<p>다양한 분류기의 결과를 결합함으로써 세분화 결과는 입력 데이터의 노이즈 및 변동성에 대해 더 큰 견고성을 얻을 수 있습니다. [7][5]</p>
<h1 id="평가지표"><a href="#평가지표" class="headerlink" title="평가지표"></a>평가지표</h1><p>주요 지표에는 정밀도, 재현율, 정확도, F1 점수, Intersection over Union(IoU), Boundary F1 점수 등이 있습니다. 이 항목 외에도,</p>
<ul>
<li>알고리즘 학습과 실행에 필요한 시간을 모두 포함하는 분할 방법의 계산 효율성을 평가하는 것이 필수적입니다.[7]</li>
<li>세분화 방법은 성능에 대한 포괄적인 평가를 보장하기 위해 정밀도, 재현율, 정확도 및 효율성 요소를 기준으로 비교해야 합니다.[7]</li>
<li>또한 연구자들은 전문가 지식을 주석 품질에 통합하고 Dice 계수와 같은 기존 방법을 넘어서는 새로운 지표를 개발하는 것과 같이 세분화 정확도의 더 복잡한 측면을 포착하기 위해 평가 지표를 지속적으로 개선하고 있습니다.[11]</li>
</ul>
<h1 id="도전-과제"><a href="#도전-과제" class="headerlink" title="도전 과제"></a>도전 과제</h1><p>이미지 분할은 실제 시나리오에서 효과성과 적용성을 방해하는 다양한 과제에 계속 직면하고 있습니다. 이러한 과제는 데이터 가변성과 관련된 문제부터 분할 알고리즘의 계산적 요구 사항까지 다양합니다.</p>
<h2 id="Partial-occlusion"><a href="#Partial-occlusion" class="headerlink" title="Partial occlusion"></a>Partial occlusion</h2><p>이미지에서 물체의 부분적 폐색 현상이다. 여러 프레임으로 이루어진 이미지에서 장애물로 인해 물체의 일부만 보니는 현상이다. </p>
<img src='https://www.researchgate.net/profile/Kang-Hyun-Jo/publication/257645271/figure/fig2/AS:297626177163268@1447971018494/ehicle-partial-occlusion-and-temporarily-missing-example.png'>

<ul>
<li>A Novel Particle Filter Implementation for a Multiple-Vehicle Detection and Tracking System Using Tail Light Segmentation</li>
</ul>
<p>이는 명확한 경계를 정의하는 작업을 복잡하게 만들어 크기, 모양 및 특징 측정에 오류가 발생하는 경우가 많습니다.[6]</p>
<h2 id="여러-개체-포함시-임계값-조정-문제"><a href="#여러-개체-포함시-임계값-조정-문제" class="headerlink" title="여러 개체 포함시 임계값 조정 문제"></a>여러 개체 포함시 임계값 조정 문제</h2><p>또한 이미지에 다양한 모양, 크기 및 강도를 가진 여러 개체 유형을 포함하는 경우 다양한 개체에 대한 임계값을 조정하는 것이 복잡할 수 있으므로 상당한 어려움을 나타냅니다.[6]</p>
<img src='https://bioimagebook.github.io/_images/ed03f1ec7feebd7c8aa9332c5b020834dedfe1081014002e8ede82172e6cdaf0.png'>
 - https://bioimagebook.github.io/chapters/2-processing/3-thresholding/thresholding.html




<h2 id="조명-및-컨텍스트-가변성"><a href="#조명-및-컨텍스트-가변성" class="headerlink" title="조명 및 컨텍스트 가변성"></a>조명 및 컨텍스트 가변성</h2><p>다양한 조명 조건도 효과적인 분할에 상당한 어려움을 줍니다. 고르지 않거나 균일하지 않은 조명에서 촬영한 이미지는 물체가 배경과 효과적으로 분리되지 않을 수 있으므로 부정확할 수 있습니다. [6]</p>
<img src='https://www.researchgate.net/publication/326430739/figure/fig9/AS:1086468067987470@1636045581183/The-low-light-images-and-the-corresponding-image-segmentation-results-Top-low-light.jpg'>
 - A Low-Light Image Enhancement Method Based on Image Degradation Model and Pure Pixel Ratio Prior


<p>또한 이미지 내에서 점진적인 강도 전환이 존재하면 기존 방법은 일반적으로 잘 정의된 모서리에 최적화되어 있기 때문에 정확한 분할을 달성하기 어려울 수 있습니다.[6]</p>
<h2 id="높은-계산-능력"><a href="#높은-계산-능력" class="headerlink" title="높은 계산 능력"></a>높은 계산 능력</h2><p>또 다른 중요한 장애물은 많은 고급 세분화 기술, 특히 훈련 단계에서 필요한 높은 연산 능력입니다. 이러한 높은 연산 오버헤드는 빠른 응답이 중요한 실시간 처리 시나리오에서 특정 방법의 적용성을 제한할 수 있습니다.[ 9 ]</p>
<p>모델 가지치기, 양자화 및 GPU나 TPU와 같은 특수 하드웨어 가속기 활용과 같은 기술은 이러한 제한을 완화하는 데 도움이 될 수 있지만 상당한 리소스에 대한 필요성은 여전히 ​​장애물로 남아 있습니다.[ 9 ]</p>
<h2 id="AI-윤리-고려"><a href="#AI-윤리-고려" class="headerlink" title="AI 윤리 고려"></a>AI 윤리 고려</h2><p>투명하고 윤리적이며 책임감 있는 AI 구현은 환자의 신뢰와 안전을 유지하는 데 매우 중요합니다.</p>
<ul>
<li><p><strong>데이터 프라이버시</strong>: AI 모델 훈련에는 대량의 환자 데이터가 필요하며, 이러한 데이터는 개인의 민감한 건강 정보를 포함하고 있습니다. 따라서 데이터 프라이버시를 보호하는 것은 매우 중요하며, 데이터 익명화, 접근 제어, 보안 강화 등의 조치가 필요합니다. [19]</p>
</li>
<li><p><strong>훈련 데이터의 편향</strong>: AI 모델은 훈련 데이터의 편향을 반영할 수 있으며, 이는 특정 인종, 성별, 사회경제적 배경의 환자에게 불공정한 결과를 초래할 수 있습니다. 따라서 다양하고 대표적인 데이터 세트를 사용하여 훈련 데이터의 편향을 최소화하고, 공정성을 확보하는 것이 중요합니다. [19]</p>
</li>
<li><p><strong>AI 기반 결정의 해석 가능성</strong>: 심층 학습 모델은 종종 “블랙 박스”로 여겨지며, 의사 결정 과정을 이해하기 어려울 수 있습니다. 의료 분야에서는 의사가 AI 모델의 결정을 이해하고 신뢰할 수 있어야 하므로, 설명 가능한 AI(XAI) 기술 개발이 중요합니다. 설명 가능한 AI는 AI 모델의 의사 결정 과정을 투명하게 공개하고, 의료 전문가가 AI 모델의 판단 근거를 이해하고 검증할 수 있도록 지원합니다. [19]</p>
</li>
</ul>
<h1 id="미래-방향"><a href="#미래-방향" class="headerlink" title="미래 방향"></a>미래 방향</h1><p>이미지 분할 분야의 개선 및 혁신 분야에 초점을 맞춰 이미지 분할의 잠재적인 미래 방향을 논의합니다.</p>
<h2 id="딥러닝-통합"><a href="#딥러닝-통합" class="headerlink" title="딥러닝 통합"></a>딥러닝 통합</h2><p>미래 연구의 유망한 방향은 세분화 성능을 향상시키기 위해 다양한 딥 러닝 모델을 결합하는 것입니다. 신경망 아키텍처의 지속적인 발전은 하이브리드 모델이 여러 접근 방식의 강점을 활용하여 더 우수한 결과를 제공할 수 있음을 시사합니다. 이러한 통합은 다양한 영상 조건 및 모달리티에 더 적합한 보다 견고하고 일반화 가능한 세분화 알고리즘의 개발로 이어질 수 있습니다.[17][8]</p>
<h2 id="크로스-도메인-적응"><a href="#크로스-도메인-적응" class="headerlink" title="크로스 도메인 적응"></a>크로스 도메인 적응</h2><p>교차 도메인 세분화는 여전히 중요한 과제로 남아 있으며, 특히 영상 기술의 변화가 성능에 상당한 영향을 미칠 수 있는 의료 영상 분야에서 그렇습니다. 향후 연구에서는 도메인 이동의 영향을 완화하고 다양한 데이터 세트에서 세분화 모델의 효능을 개선하는 것을 목표로 하는 새로운 도메인 적응 기술을 탐색할 수 있습니다. [8]</p>
<p>연구자들은 보다 풍부한 맥락적 이해를 제공하고 향상된 세분화 정확도로 이어질 수 있는 다중 모달 이미징 데이터의 통합을 조사하도록 권장됩니다.[18]</p>
<h2 id="실시간-처리"><a href="#실시간-처리" class="headerlink" title="실시간 처리"></a>실시간 처리</h2><p>실시간 처리 기능에 대한 수요는 특히 자율 주행차 및 의료 진단과 같은 애플리케이션에서 증가하고 있습니다. 미래의 이미지 분할 프레임워크는 정확도를 손상시키지 않으면서 실시간 실행을 위한 알고리즘 최적화에 집중해야 합니다 이를 위해서는 실시간 분석의 높은 요구 사항을 처리할 수 있는 보다 효율적인 계산 모델을 개발해야 합니다. [1]</p>
<h2 id="학제간-시너지-Synergy-with-Other-AI-Domain"><a href="#학제간-시너지-Synergy-with-Other-AI-Domain" class="headerlink" title="학제간 시너지(Synergy with Other AI Domain)"></a>학제간 시너지(Synergy with Other AI Domain)</h2><p>이미지 분할과 자연어 처리(NLP) 및 증강 현실과 같은 다른 AI 기술의 교차점은 혁신을 위한 비옥한 토양을 제공합니다. 시스템이 시각 데이터와 인간 언어를 모두 해석하고 이해할 수 있도록 함으로써 연구자는 사용자 상호 작용 및 의사 결정 프로세스를 향상시키는 보다 포괄적인 AI 애플리케이션을 만들 수 있습니다. [1]</p>
<h2 id="설명-가능한-Explainable-AI"><a href="#설명-가능한-Explainable-AI" class="headerlink" title="설명 가능한 (Explainable) AI"></a>설명 가능한 (Explainable) AI</h2><p>AI가 의료 분야에 계속 침투함에 따라 설명 가능한 AI에 대한 필요성이 가장 중요해졌습니다. 세분화 기술의 미래 발전은 투명성을 우선시해야 하며, 의료 전문가가 진단 프로세스를 주도하는 알고리즘을 이해하고 신뢰할 수 있도록 해야 합니다. 설명 가능성에 대한 이러한 초점은 의학과 같은 민감한 분야에서 AI 기술을 책임감 있게 배포하는 데 매우 중요할 것입니다. [19]</p>
<hr>
<h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><p>[4] 이미지 분할 - 위키피디아, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Image_segmentation">https://en.wikipedia.org/wiki/Image_segmentation</a></p>
<p>[5]  의미론적 이미지 분할의 최근 진전 - arXiv.org: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.10198">https://arxiv.org/abs/1809.10198</a></p>
<p>[6] 이미지 처리에서의 이미지 임계값 설정 - Encord: <a target="_blank" rel="noopener" href="https://encord.com/blog/image-thresholding-image-processing/">https://encord.com/blog/image-thresholding-image-processing/</a></p>
<p>[7] …대한 이미지 분할 알고리즘의 성능 평가: <a target="_blank" rel="noopener" href="https://www.academia.edu/88049601/%EB%AF%B8%EC%8B%9C%EC%A0%81">https://www.academia.edu/88049601/미시적</a> 이미지 데이터에 대한 이미지 분할 알고리즘의 성능 평가</p>
<p>[8] 뇌 의학적 분야에서 여러 영역의 과제를 해결하기 위한 접근법 탐구 : <a target="_blank" rel="noopener" href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1401329/full">https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1401329/full</a></p>
<p>[11] 컴퓨터 비전에서의 이미지 분할 가이드: 모범 사례 - Encord : <a target="_blank" rel="noopener" href="https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/">https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/</a></p>
<p>[17] An overview of intelligent image segmentation using active contour models: <a target="_blank" rel="noopener" href="https://www.oaepublish.com/articles/ir.2023.02">https://www.oaepublish.com/articles/ir.2023.02</a></p>
<p>[19] Medical Image Segmentation and Its Real-World Applications: <a target="_blank" rel="noopener" href="https://medium.com/@jervisaldanha/medical-image-segmentation-and-its-real-world-applications-unet-and-beyond-9cd06eeebcb6">https://medium.com/@jervisaldanha/medical-image-segmentation-and-its-real-world-applications-unet-and-beyond-9cd06eeebcb6</a></p>
<p>[A1] ISBI 2015, <a target="_blank" rel="noopener" href="http://brainiac2.mit.edu/isbi_challenge/">http://brainiac2.mit.edu/isbi_challenge/</a><br>[A2] <a target="_blank" rel="noopener" href="http://lmb.informatik.uni-freiburg.de/people/ronneber/isbi2015/">http://lmb.informatik.uni-freiburg.de/people/ronneber/isbi2015/</a><br>[A3] “Machine Perception of Three-Dimensional Solids”, Larry Roberts, : <a target="_blank" rel="noopener" href="https://bit.ly/3ZPSBHF">https://bit.ly/3ZPSBHF</a><br>[A4] “The Dawn of Computer Vision:”, <a target="_blank" rel="noopener" href="https://www.turingpost.com/p/cvhistory2">https://www.turingpost.com/p/cvhistory2</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-01-15T08:00:00.000Z" title="1/15/2023, 5:00:00 PM">2023-01-15</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2023-01-15T09:01:14.000Z" title="1/15/2023, 6:01:14 PM">2023-01-15</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a></span><span class="level-item">33분안에 읽기 (약 4938 단어)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EC%98%81%EC%83%81%EB%B6%84%EC%95%BC%EC%97%90%EC%84%9C%EC%9D%98_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%EB%B0%9C%EB%8B%AC_%EB%8B%A8%EA%B3%84%EC%97%90_%EB%94%B0%EB%A5%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80%EB%AA%A8%EB%8D%B8%EC%9D%98_%EB%B3%80%ED%99%94,IITP,%EC%A3%BC%EA%B0%84%EA%B8%B0%EC%88%A0%EB%8F%99%ED%96%A52071%ED%98%B8-af3a1ae10287/">[요약] 영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화</a></p><div class="content"><p>다음 2개의 글을 요약하고 설명을 추가로 검색해 요약해 두었다.</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.iitp.kr/kr/1/knowledge/periodicalViewA.it?searClassCode=B_ITA_01&masterCode=publication&identifier=1171">인공지능 학습용 영상 데이터 기술 동향, IITP 주간기술동향 1988호</a>, 임철홍</li>
<li><a target="_blank" rel="noopener" href="https://iitp.kr/kr/1/knowledge/periodicalViewA.it?masterCode=publication&searClassCode=B_ITA_01&identifier=1256">영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화, IITP 주간기술동향 2071호</a>, 김혜진_한국전자통신연구원 책임연구원</li>
</ol>
<blockquote>
<p>2023&#x2F;01&#x2F;15 요약 작성</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="요약-영상-분야에서의-인공지능-발달-단계에-따른-데이터와-모델의-변화"><a href="#요약-영상-분야에서의-인공지능-발달-단계에-따른-데이터와-모델의-변화" class="headerlink" title="[요약] 영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화"></a>[요약] 영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화</h1><p><a target="_blank" rel="noopener" href="https://iitp.kr/kr/1/knowledge/periodicalViewA.it?masterCode=publication&searClassCode=B_ITA_01&identifier=1256">IITP 주간기술동향 2071호등록자 &#x2F; 영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화</a></p>
<ul>
<li>김혜진_한국전자통신연구원 책임연구원</li>
</ul>
<h1 id="I-발전"><a href="#I-발전" class="headerlink" title="I. 발전"></a>I. 발전</h1><p><strong><em>1세대</em></strong></p>
<p>1950년대에서 80년대에 이르기까지의 규칙ㆍ지식에 기반을 둔 추론 시스템</p>
<p><strong><em>2세대</em></strong></p>
<p>제프리 힌튼(Geoffrey Hinton)과 얀 리쿤(Yann LeCun), 요슈아 벤지오(Yoshua Bengio)에 의해 시작된 특정 문제에 국한된 데이터셋으로부터 학습을 통해 습득하는 AI 2세대라 할 수 있다</p>
<p><strong><em>3세대</em></strong></p>
<p>범용적인 문제를 해결할 수 있는 인공지능</p>
<h2 id="인공지능-하이프-사이클"><a href="#인공지능-하이프-사이클" class="headerlink" title="인공지능 하이프 사이클"></a>인공지능 하이프 사이클</h2><img src='/images/bigdata/gartner_ai_hipe_cycle.png'>

<h1 id="II-AI-2세대의-데이터"><a href="#II-AI-2세대의-데이터" class="headerlink" title="II. AI 2세대의 데이터"></a>II. AI 2세대의 데이터</h1><p>최근 2세대 AI 들은 널리 알려진 형태로, 다양한 분야에서 데이터를 모으려는 노력을 바탕으로 발전되고 있다. 다양한 공개 데이터세트를 비교해 공개된 데이터셋에서 성능에 대한 비교를 포함하는 것이 거의 필수 요소가 되었다.</p>
<h2 id="이미지-분류-데이터-세트"><a href="#이미지-분류-데이터-세트" class="headerlink" title="이미지 분류 데이터 세트"></a>이미지 분류 데이터 세트</h2><p><a target="_blank" rel="noopener" href="https://www.iitp.kr/kr/1/knowledge/periodicalViewA.it?searClassCode=B_ITA_01&masterCode=publication&identifier=1171">인공지능 학습용 영상 데이터 기술 동향, IITP, 주간기술동향 1988</a> 에 정리</p>
<ul>
<li><p>MNIST</p>
<p>숫자 10종류에 대해 7만 장의 이미지로 구성되어 있다</p>
</li>
<li><p>ImageNet</p>
<p>영상 downstream task에서 pretrained network로 사용되고 있는 데이터셋 중 하나로, 1,000종류, 14,197,122의 이미지로 구성되어 있다.</p>
</li>
<li><p>CIFAR</p>
<p>CIFAR-10, CIFAR-100으로 각각 10종류, 600장과 100종류,60,000장의 이미지가 있다</p>
</li>
</ul>
<h2 id="영상에서-객체-검출-데이터-세트"><a href="#영상에서-객체-검출-데이터-세트" class="headerlink" title="영상에서 객체 검출 데이터 세트"></a>영상에서 객체 검출 데이터 세트</h2><p>이미지의 상황을 이해하여 캡션 등을 자동으로 생성하기 위한 연구가 진행되면서 다중 객체 인식을 기반으로 장면 설명, 객체 간의 관계 등의 데이터가 필요하게 되었다</p>
<ul>
<li>MS COCO[1], PASCAL VOC 2012[2] 등이 객체 검출을 목적으로 구축된 데이터셋이다.</li>
<li>구글 Open Image</li>
<li>STANDFORD와 YAHOO의 Visual Genome</li>
</ul>
<p>객체 검출 알고리즘으로 널리 알려진 R-CNN, YOLO 계열의 알고리즘들도 모두 이 데이터셋을 기반으로 개발되었다.[1][2]</p>
<h3 id="MS-COCO-Common-Object-in-COntext"><a href="#MS-COCO-Common-Object-in-COntext" class="headerlink" title="MS COCO (Common Object in COntext)"></a>MS COCO (Common Object in COntext)</h3><p>이미지의 객체 인식, 분할, 캡션 인식을 위한 공개된 데이터 셋이다. 330,000개의 이미지에서 80개 분류 1,500,000개의 객체 인스턴스를 가지고 있다. Flickr의 이미지를 기반으로 학습과 테스트가 진행되었다</p>
<img src='/images/bigdata/coco_object_detect.png'>

<p>COCO 홈페이지 데이터 셋 메뉴에서 explorer를 선택하면 직접 데이터 셋을 볼 수가 있는데, 위 그림 처럼 선택된 분류 객체가 분할된 이미지를 볼 수 있다. 앞에 있는 것은 ‘person’이 선택되어 사람 객체가 분할되어 보이며, 뒤에 있는 것은 ‘car’가 선택되어 자동차 객체가 분할되어 보인다. [2]</p>
<p>데이터 셋은 이미지 원본파일과 이를 설명하는 annotation 파일로 구성된다. annotation 파일은 captions, instances, person_keypoints 파일로 구성되며,json 형태로 되어 있다. 각 json 파일은 전체 이미지에 대해 하나로 구성되어 있어 크기 매우 크다.</p>
<p>annotation 에는 개체에 대한 정보가 info, license, images, annotations, categories 등으로 제공된다.</p>
<img src='/images/bigdata/coco_object_annotation.png'>

<ul>
<li><a target="_blank" rel="noopener" href="https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch">https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch</a></li>
</ul>
<h3 id="Open-Image"><a href="#Open-Image" class="headerlink" title="Open Image"></a>Open Image</h3><p>Open Image는 구글에서 공개한 오픈 이미지 데이터 셋이다. 이미지 수준 레이블(image-level labels), 객체 경계 상자(object bounding boxes), 객체 분할 마스크(segmentation masks), 시각적 관계(visual relationships), 나레이션(localized narratives)을 포함하는 데이터 이다.</p>
<ul>
<li>2016년 처음 공개<ul>
<li><a target="_blank" rel="noopener" href="https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html">Introducing the Open Images Dataset</a></li>
</ul>
</li>
<li>2018년 V4<ul>
<li><a target="_blank" rel="noopener" href="https://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html">Announcing Open Images V4 and the ECCV 2018 Open Images Challenge</a></li>
</ul>
</li>
<li>2020년 2월 V6<ul>
<li><a target="_blank" rel="noopener" href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html">Open Images V6 — Now Featuring Localized Narratives</a></li>
</ul>
</li>
</ul>
<h4 id="Open-Image-V4"><a href="#Open-Image-V4" class="headerlink" title="Open Image V4"></a>Open Image V4</h4><p>Open Image V4는 9,178,275개의 이미지에서 30,113,078개의 이미지 수준 레이블과 15,440,132개의 객체 경계 상자를 가지고 있으며, 374,768개의 시각적 관계를 나타내고 있다.</p>
<ul>
<li>Flickr 에서 고해상도, creative common, crowd sourcing 라이센스 위주로 수집</li>
<li>이미지 분류는 구글 데이터셋 JFT 의 19,794개 분류 체계</li>
<li><a target="_blank" rel="noopener" href="https://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html">Announcing Open Images V4 and the ECCV 2018 Open Images Challenge</a></li>
</ul>
<h4 id="Bounding-Box"><a href="#Bounding-Box" class="headerlink" title="Bounding Box"></a>Bounding Box</h4><p>객체 경계 상자는 이미지에서 객체 인식에 활용되기 위한 정보이다.</p>
<img src='https://1.bp.blogspot.com/-_-5Bxwk5DPA/WW_kvncLWmI/AAAAAAAAB6U/3ObwsFWyx-Yq7sD_Ea0NOYx65iTWyhQJACLcBGAs/s1600/f1.png'>
    그림 - google ai blog

<p>바운딩 박스를 통해서 객체 경계 상자에 나타난 객체들은 서로 관계를 맺고 있으며, Open Image에서 이들의 시각적 관계가 아래 같이 같이 점선박스로 보여진다.</p>
<img src='https://1.bp.blogspot.com/-yuodfZa6gyM/XlbQfiAzbzI/AAAAAAAAFYA/QSTnuZksQII2PaRON2mqHntZBHL-saniACLcBGAsYHQ/s640/Figure1.png'>
그림 - google ai blog: https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html

<h4 id="Open-Image-V5"><a href="#Open-Image-V5" class="headerlink" title="Open Image V5"></a>Open Image V5</h4><p>Open Image V5에서는 350개 카테고리의 2,800,000개의 객체 분할 마스크가 추가되었다</p>
<h4 id="Open-Image-V6"><a href="#Open-Image-V6" class="headerlink" title="Open Image V6"></a>Open Image V6</h4><p>Open Image V6에서는 675,000개의 나레이션이 추가되었다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html">Open Images V6 — Now Featuring Localized Narratives</a></li>
</ul>
<p>나레이션은 이미지를 설명하는 캡션과 음성 설명이 포함되어 있고 음성 설명과 캡션에 해당하는 사물이나 동작등을 마우스로 그린 트레이스가 포함되어 있다</p>
<img src='https://1.bp.blogspot.com/-ksHz0vF4L5E/XjShRwSzNuI/AAAAAAAAFPY/37QrnykLGE8uJwMy5ncv8wCl8rRPdZaKACEwYBhgL/s400/image4.png'>

<h3 id="Visual-Genome"><a href="#Visual-Genome" class="headerlink" title="Visual Genome"></a>Visual Genome</h3><p>Visual Genome은 지식 베이스의 이미지 데이터 셋으로 이미지의 구조를 언어와 연결하려고 노력하고 있다. 108,077개의 이미지에 5,400,000개의 지역 설명과 3,800,000 개의 객체, 2,800,000개의 속성, 2,300,000개의 관계로 구성되어 있다.</p>
<p>데이터 셋은 지역 설명(region descriptions), 객체(Objects), 속성(attributes), 관계(relationships), 지역 그래프(region graphs), 장면 그래프 (Scenegraphs) 및 질문답변으로 구성되어 있다</p>
<img src='https://production-media.paperswithcode.com/datasets/Visual_Genome-0000000087-aaf04589_6fjxO1x.jpg'>
그림 - https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision/

<p>지역 설명은 객체의 상태나 동작을 나타내고 있으며, 이들은 객체와 속성으로 나누어 설명되며, 각각 그래프의 형태로 간단하게 표현될 수 있다. 이미지의 여러 지역(region)은 합쳐져서 전체 장면 설명을 하는 장면 그래프로 표현된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="의미론적-분할-Semantic-Segmentation"><a href="#의미론적-분할-Semantic-Segmentation" class="headerlink" title="의미론적 분할(Semantic Segmentation)"></a>의미론적 분할(Semantic Segmentation)</h2><p>이미지 분류와 객체 검출과도 연관이 깊은분야로, Cityscapes[3], ADE20K[4], PASCAL VOC2012[5] 등이 있다</p>
<ul>
<li>Cityscapes는 도시 환경에서의 의미론적 분할을 ADE20K는 sky, road, grass, person,car, bed 등 150종목에 대한 장면 중심의 영상 분할을 다룬다.</li>
<li>PASCAL Context는 PASCALVOC 2010의 확장판으로 400종 이상의 레이블을 제공한다.</li>
</ul>
<img src='/images/bigdata/synthetic_data_example.png'>

<h3 id="거리-추정"><a href="#거리-추정" class="headerlink" title="거리 추정"></a>거리 추정</h3><p>이미지와 영상에서 빼놓을 수 없는 분야 중 하나로 실외 거리 추정을 위해서는 KITTI 데이터셋[6]이 실내는 NYU v2[7] 데이터셋이 널리 활용되어 왔다</p>
<h3 id="이미지-생성-분야"><a href="#이미지-생성-분야" class="headerlink" title="이미지 생성 분야"></a>이미지 생성 분야</h3><p>2세대 AI는 데이터가 충분할수록 성능 향상을 기대할 수 있는 학습 기반 인공지능으로 <strong>이미지 생성</strong> 분야는 이러한 데이터를 “생성” 하는 데 활용을 기대할 수 있기 때문에 데이터 관점에서 2세대 AI에서의 이룬 괄목할 만한 성과 중 하나라 할 수 있다.</p>
<p>널리 알려진 이미지 생성 데이터셋으로는</p>
<ul>
<li><p>CelebA</p>
<p>10,177명의 유명인사에 대한 202,599 얼굴이미지로 이루어졌다.</p>
</li>
<li><p>FFHQ</p>
<p>연령, 인종, 다양한 배경 변화를 가진 70,000 고해상도 영상들로 이루어졌다.</p>
</li>
</ul>
<h1 id="III-데이터-부족에-대한-논의"><a href="#III-데이터-부족에-대한-논의" class="headerlink" title="III. 데이터 부족에 대한 논의"></a>III. 데이터 부족에 대한 논의</h1><p>기존에 AI 연구는 빅데이터를 가정하고 있으나 실제 데이터 분석 사례에서는 빅데이터가 아닌 경우가 많다.데이터가 적고 레이블에 일관성이 없는 경우에는 모델을 아무리 개선을 한다고 해도(즉, model-centric 접근 방식을 적용) 성능을 개선하기 어렵다는 것을 2021년 3월 25일 Deep Learning AI에서 주최한 앤드류 응 교수가 보여주었다.</p>
<img src='/images/bigdata/model_centric-data-centric.png'>

<h3 id="대응"><a href="#대응" class="headerlink" title="대응"></a>대응</h3><ol>
<li><p>이러한 문제를 해결하기 위해 컴퓨터 비전 분야의 저명한 학회인 CVPR(Computer Vision and Pattern Recognition) 학회에서는 2020년부터 2022년에 걸쳐 limited labeled data와 관련된 워크샵을 다루었으며,</p>
</li>
<li><p>합성 데이터를 만들어 데이터 부족 문제를 극복하려는 많은 시도가 있었다. BMW와 같은 자동차 제조업체뿐만 아니라, 은행, 공장, 병원, 로봇 등 다양한 분야에서 이러한 합성 데이터를 기반으로 AI 모델 학습에 적용하고 있다</p>
</li>
</ol>
<img src='/images/bigdata/synthetic_data_example.png'>

<h3 id="가트너-미래는-합성데이터"><a href="#가트너-미래는-합성데이터" class="headerlink" title="가트너, 미래는 합성데이터"></a>가트너, 미래는 합성데이터</h3><p>2021년 6월 보고서에 따르면 2030년에는 AI의 대부분의 데이터가 합성 데이터를 기반으로 생성될 것으로 보고, 2024년까지 AI 및 분석에 사용되는 데이터의 60%가 이 합성데이터를 사용할 것이라고 예측</p>
<img src='/images/bigdata/gartner_synthetic.png'>

<p>실제 데이터를 얻는 비용이 인건비 수준에서 많게는 수십억에 이르는 문제가 있기 때문이다. 반면에, 합성 데이터를 만들어 더욱 정확한 레이블을 얻을 수 있는 경우도 많다.</p>
<p>예를 들어, 거리 추정 데이터의 경우 센서의 정확도가 한계가 있어 픽셀에 매핑되는 거리 레이블의 정확도가 떨어진다. 반면, 합성 데이터의 경우에는 모든 픽셀에서 높은 정확도의 거리 레이블링 데이터를 얻을 수 있다. 즉, 합성데이터는 앞서 언급한 앤드류 응 교수가 지적한 레이블의 품질 저하 문제를 일으키지 않기 때문에 더 정확한 모델 학습이 가능할 수 있다.</p>
<h3 id="합성데이터"><a href="#합성데이터" class="headerlink" title="합성데이터"></a>합성데이터</h3><p>합성 데이터를 얻는 방법에는 시뮬레이션으로 얻는 방법, AI 기법(GAN, VAE, Normalizing Flow) 또는 도메인 랜덤화 등이 널리 알려져 있다. 최근에는 확산 모델(Diffusion model)[15]과 NeRF[16]의 등장으로 한층 더 정교해졌다</p>
<p>대표적인 확산 모델로</p>
<ol>
<li>오픈 AI의 DALL-E 2의 백본 모델이 있다.</li>
<li>확산 오토인코더(Diffusion Autoencoder)와 같이 의미론적 의미가 있는 확산 모델도 제안</li>
<li>NeRF는 기존의 방법들이 시점에 대한 변화를 주기 어려웠던 점에 반해, 차량 앞면을 보고, 뒷면을 생성할 수 있는 등 다양한 시점에서의 영상을 생성할 수 있다. 더 나아가, 2D 이미지에서 3D 이미지를 생성함으로써 영상의 스케일 변화까지 줄 수 있어 데이터 합성에 있어 큰 전환점을 마련하였다</li>
</ol>
<h3 id="DALL-E-2-backbone-Model"><a href="#DALL-E-2-backbone-Model" class="headerlink" title="DALL-E 2 backbone Model"></a>DALL-E 2 backbone Model</h3><p>이미지와 텍스트의 관계를 학습하고, 이를 통해, 영상에 다양한 변화를 줄 수 있게 했을 뿐만 아니라 텍스트를 통해 고해상도의 이미지를 생성</p>
<h3 id="부족한-데이터-문제를-극복하기-위한-방법으로-자기지도학습-self-supervised-learning-방법"><a href="#부족한-데이터-문제를-극복하기-위한-방법으로-자기지도학습-self-supervised-learning-방법" class="headerlink" title="부족한 데이터 문제를 극복하기 위한 방법으로 자기지도학습(self-supervised learning) 방법"></a>부족한 데이터 문제를 극복하기 위한 방법으로 자기지도학습(self-supervised learning) 방법</h3><p>자기지도학습 방법은 비지도학습과 유사하게 레이블 없는 데이터셋에서 사용자가 직접 정의한 작업(pretext task)를 목표로 학습시키게 된다. 이 때, 이 작업은 데이터에서 레이블로 사용될 수 있는 정보를 활용하여 지도학습처럼 학습시키게 되어 데이터 부족 문제를 우회적으로 풀 수 있게 된다.</p>
<h1 id="IV-AI-3세대를-지향하는-디딤돌-데이터셋의-등장"><a href="#IV-AI-3세대를-지향하는-디딤돌-데이터셋의-등장" class="headerlink" title="IV. AI 3세대를 지향하는 디딤돌 데이터셋의 등장"></a>IV. AI 3세대를 지향하는 디딤돌 데이터셋의 등장</h1><p>자기주도학습 데이터세트</p>
<p>하나의 태스크에 국한되어 있지 않은 응용성을 가진 데이터셋이 점점 더 다양하게 등장하고 있다. 이러한 현상은 영상 내에서만 국한되지 않고, 텍스트를 포함하고 더 나아가 구조화된 데이터, 3D 신호 데이터 등 점점 더 다양한 데이터셋을 포함하는 방향으로 확장되고 있다. 이렇게 이기종의 빅데이터를 학습시키게 되면 파운데이션 모델(foundation model)을 얻을 수 있게 된다. 이러한 파운데이션 모델은 대규모 데이터로 사전학습되어 다른 모델에 지식을 전달해 줄 수 있는 모델을 의미한다</p>
<p>자기주도학습이 비지도학습과 달리 지도학습에 견줄 수 있는 성능을 획득하게 된 것은 Pretext task 단계에서 큰 데이터셋을 활용할 수 있는 덕분이다.</p>
<h2 id="KITTI-데이터셋"><a href="#KITTI-데이터셋" class="headerlink" title="KITTI 데이터셋"></a>KITTI 데이터셋</h2><p>KITTI 데이터셋은 거리 추정을 포함한 2D&#x2F;3D 객체 검출, 도로 환경에서의 의미론적 분할 정보, 주행거리계(odometry), 도로 환경에서 객체 추적, 차선 검출 등 다양한 정보를 포함하고 있다.KITTI 데이터셋이 자율주행을 위해 필요로 하는 데이터셋을 포함하고 있어 자율주행 기술 발전에 공헌한 바가 크기때문이다</p>
<h3 id="주요-블로그-글"><a href="#주요-블로그-글" class="headerlink" title="주요 블로그 글"></a>주요 블로그 글</h3><ol>
<li><a target="_blank" rel="noopener" href="https://gaussian37.github.io/vision-dataset-kitti/">KITTI 데이터 세트</a></li>
<li><a target="_blank" rel="noopener" href="https://hnsuk.tistory.com/25">KITTI 데이터 세트&#x2F;데이터 분해 설명</a></li>
</ol>
<h2 id="파운데이션-모델"><a href="#파운데이션-모델" class="headerlink" title="파운데이션 모델:"></a>파운데이션 모델:</h2><p>파운데이션 모델은 스탠포드의 인간중심 인공지능연구소에서 2021년 처음으로 대중화한 용어로 소개되었다. 그러나 파운데이션 모델의 가능성은 먼저 초거대 AI로 불리는 모델<br>들인 BERT, DALL-E 2, GPT-3로부터 시작되었다.</p>
<img src='/images/bigdata/foudation_model.png'>

<p>초거대 AI는 초기에는 언어모델에 국한되었으나, 점차 이미지를 함께 포함하는 모델로, 또는 다양한 언어를 포함하는 모델로 확장되고 있다</p>
<h2 id="초거대-AI"><a href="#초거대-AI" class="headerlink" title="초거대 AI"></a>초거대 AI</h2><p>초거대 AI는 초기에는 언어 모델에 국한되었으나, 점차 이미지를 함께 포함하는 모델로, 또는 다양한 언어를 포함하는 모델로 확장되고 있다([표 4] 참조)</p>
<img src='/images/bigdata/supernova_ai.png'>

<p>최근 발표된 GODEL은 주제를 변경하고 학습시에 주어지지 않은 이벤트에 대한 질문에 응답할 수도 있고, 구조화되지 않은 텍스트를 통해 검색할 수 있게 하며, 대화식 질문에도 응답이 가능한 모델로 인공지능 3세대의 자격요건에 한층 더 가까워졌다.</p>
<h3 id="GODEL-Grounded-Open-Dialogue-Model"><a href="#GODEL-Grounded-Open-Dialogue-Model" class="headerlink" title="GODEL(Grounded Open Dialogue Model)"></a>GODEL(Grounded Open Dialogue Model)</h3><p>가상비서나 챗봇과 같은 대화 에이전트가 레스토랑 추천과 같은 주제별 전문 지식을 제공하는 것 외에도 지역의 역사나 최근 스포츠 경기에 대한 대화에 참여할 수 있다면 어떨까? 또한 에이전트의 응답이 최근의 이벤트와 이슈를 지속적으로 반영한다면 어떨까?</p>
<p>고델은 마이크로소프트가 2019년에 발표한 최초의 대규모 사전 훈련 언어 모델인 DialoGPT의 개선된 대화형 언어 모델이다. 고델은 응답할 수 있는 쿼리 유형과 가져올 수 있는 정보 소스에 제한이 없는 대화 에이전트를 만드는 것을 목적으로 한다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/project/godel/">https://www.microsoft.com/en-us/research/project/godel/</a></li>
<li>기사&#x2F;블로그: <a target="_blank" rel="noopener" href="https://www.aitimes.com/news/articleView.html?idxno=145531">마이크로소프트, 비학습 데이터로 응답하는 언어 모델 “고델(GODEL)” 공개</a></li>
</ul>
<p>마이크로소프트에 따르면 고델은 대화 에이전트에 대화 내용 뿐만 아니라 훈련에 사용된 데이터에 포함되지 않은 외부 정보를 기반으로 응답을 생성할 수 있는 기능을 제공한다.</p>
<p>지역의 레스토랑에 대한 추천을 얘기하는 갑자기 최근에 발생한 토네이도에 대한 얘기를 했을때 웹에서 관련 정보를 가져와 응답하고 원래 주제로 돌아 가려고 하는 내용을 보여준다.</p>
<img src='https://www.microsoft.com/en-us/research/uploads/prod/2022/06/GODEL_Slide2.jpg'>
그림 - https://www.microsoft.com/en-us/research/blog/godel-combining-goal-oriented-dialog-with-real-world-conversations/

<h2 id="대용량-데이터-세트의-구축이-초거대-AI-개발-근간"><a href="#대용량-데이터-세트의-구축이-초거대-AI-개발-근간" class="headerlink" title="대용량 데이터 세트의 구축이 초거대 AI 개발 근간"></a>대용량 데이터 세트의 구축이 초거대 AI 개발 근간</h2><p>초거대 AI의 개발은 근간이 되는 대용량 데이터셋이 구축된 덕분이다.</p>
<ul>
<li>구글의 경우 18억 건의 데이터셋을 구축했고</li>
<li>오픈 AI의 경우 10억 건 수준으로 알려져 있다[25].</li>
<li>카카오 브레인은 정제를 거친 20억 건 수준의 이미지ㆍ테스트 데이터를 구축하고 있다[26].</li>
</ul>
<p>영상을 중심으로 하는 파운데이션 모델은 비전-언어 사전학습 모델(Vision-Language Pretraining:VLP)의 형태로 CLIP, Florence, CoCa 등이 알려져 있다. Open AI의 CLIP은 이미지와 자연어 4억 개 쌍의 관계를 학습한 것이다.</p>
<p>마이크로소프트의 Florence 모델은 30억 개의 이미지-텍스트 쌍의 데이터에 이 중 필터링을 통해 9억 쌍을 얻은 FLOD-9M 데이터셋을 구축하여 학습한 모델이다.</p>
<p>구글의 CoCa는 다양한 벤치마크에서 우수한 성능을 보였을 뿐만 아니라 ImageNet에서의 Zero-shot 성능이 86.3%로 매우 우수한 성능을 얻었다. Zero-shot에서의 우수한 성능은 다양한 하위 과제에서 높은 성능을 얻을 가능성을 보여준다. 즉, 영상만의 대용량보다는 언어 데이터와 쌍을 이루어 학습함으로써 더욱 좋은 표현력을 얻을 수 있게 되었다.</p>
<h2 id="양질의-충분한-데이터-문제"><a href="#양질의-충분한-데이터-문제" class="headerlink" title="양질의 충분한 데이터 문제"></a>양질의 충분한 데이터 문제</h2><p>인공지능 모델은 양질의 데이터만 충분하다면 문제<br>를 해결할 수 있다는 생각이 널리 퍼져 있다. 한편, 양질의 데이터는 비용 문제, 레이블링의<br>품질 문제, 보안 등으로 충분한 확보가 어려움도 널리 공감을 받고 있다. 합성 데이터 알고리<br>즘들의 발전과 파운데이션 모델에 기반하여 적응(adaptation)에 필요한 적은 데이터만 확보<br>하면 되도록 하는 기술의 발전으로 제3세대 인공지능은 스스로 문제에 대한 데이터를 확보 할 수 있는 AI로 한 걸음씩 다가가고 있다</p>
<hr>
<p>[1]: <a target="_blank" rel="noopener" href="https://www.iitp.kr/kr/1/knowledge/periodicalViewA.it?searClassCode=B_ITA_01&masterCode=publication&identifier=1171">인공지능 학습용 영상 데이터 기술 동향, IITP 주간기술동향 1988호</a>, 임철홍</p>
<p>[2]: <a target="_blank" rel="noopener" href="https://iitp.kr/kr/1/knowledge/periodicalViewA.it?masterCode=publication&searClassCode=B_ITA_01&identifier=1256">영상 분야에서의 인공지능 발달 단계에 따른 데이터와 모델의 변화, IITP 주간기술동향 2071호</a>, 김혜진_한국전자통신연구원 책임연구원</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/images/qkboo_400.png" alt="Gangtai"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Gangtai</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Paju, South Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">177</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">258</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/thinkbee" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/thinkbee"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/gangtaigoh"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-06-24T23:00:00.000Z">2025-06-25</time></p><p class="title"><a href="/WSL-%EC%99%B8%EB%B6%80%EB%94%94%EC%8A%A4%ED%81%AC-a38220eecf9e/">WSL 외부디스크 마운트</a></p><p class="categories"><a href="/categories/OS/">OS</a> / <a href="/categories/OS/Windows/">Windows</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-19T00:00:00.000Z">2025-04-19</time></p><p class="title"><a href="/nbconvert%EC%82%AC%EC%9A%A9-18b07cfb01b1/">nbconvert 사용</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-11T00:00:00.000Z">2025-04-11</time></p><p class="title"><a href="/llm-02LLM%ED%99%9C%EC%9A%A9-6bf20c6c19f9/">LLM02 - 활용방법</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/LLM/">LLM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-04-10T00:00:00.000Z">2025-04-10</time></p><p class="title"><a href="/llm-01LLM_Models-806075edcbe4/">LLM01 - Models</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/LLM/">LLM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-25T00:00:00.000Z">2025-03-25</time></p><p class="title"><a href="/pyenv-doctor-ea932b0ee5ca/">pyenv-doctor</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1465716454138955" data-ad-slot="4441624809" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ETC/"><span class="level-start"><span class="level-item">ETC</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/IT/"><span class="level-start"><span class="level-item">IT</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/OS/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">57</span></span></a><ul><li><a class="level is-mobile" href="/categories/OS/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/OS/Raspberry-Pi/"><span class="level-start"><span class="level-item">Raspberry Pi</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/categories/OS/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">93</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Android/"><span class="level-start"><span class="level-item">Android</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Angularjs/"><span class="level-start"><span class="level-item">Angularjs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Nodejs/"><span class="level-start"><span class="level-item">Nodejs</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">31</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/R/"><span class="level-start"><span class="level-item">R</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/git/"><span class="level-start"><span class="level-item">git</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming-Python/"><span class="level-start"><span class="level-item">Programming, Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/github/"><span class="level-start"><span class="level-item">github</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/thinkbee_logo.png" alt="IT Tech blogging" height="28"></a><p class="is-size-7"><span>&copy; 2025 Gangtai Goh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-right",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>